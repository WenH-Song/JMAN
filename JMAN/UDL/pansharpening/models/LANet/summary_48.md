# Summary
```
--------------------------------------------------------------------------------------------------------------------------------

         Layer (type)               Output Shape         Param 
================================================================
            Conv2d-1           [-1, 64, 48, 48]           2,368
            Conv2d-2           [-1, 64, 48, 48]         102,464
              ReLU-3           [-1, 64, 48, 48]               0
            Conv2d-4           [-1, 64, 48, 48]         102,464
          ResBlock-5           [-1, 64, 48, 48]               0
            Conv2d-6           [-1, 64, 48, 48]         102,464
            Conv2d-1           [-1, 64, 48, 48]           2,368              
              ReLU-7           [-1, 64, 48, 48]               0

            Conv2d-8           [-1, 64, 48, 48]         102,464
          ResBlock-9           [-1, 64, 48, 48]               0
           Linear-10             [-1, 256, 576]         332,352
        LayerNorm-11             [-1, 256, 576]           1,152
            Conv2d-2           [-1, 64, 48, 48]         102,464
           Linear-12            [-1, 256, 1728]         995,328
          Softmax-13          [-1, 8, 256, 256]               0
              ReLU-3           [-1, 64, 48, 48]               0
           Linear-14             [-1, 256, 576]         332,352
            Conv2d-4           [-1, 64, 48, 48]         102,464
          Dropout-15             [-1, 256, 576]               0
          ResBlock-5           [-1, 64, 48, 48]               0
    SelfAttention-16             [-1, 256, 576]               0
          PreNorm-17             [-1, 256, 576]               0            
            Conv2d-6           [-1, 64, 48, 48]         102,464

              ReLU-7           [-1, 64, 48, 48]               0
        LayerNorm-18             [-1, 256, 576]           1,152
            Conv2d-8           [-1, 64, 48, 48]         102,464
           Linear-19            [-1, 256, 2304]       1,329,408
          ResBlock-9           [-1, 64, 48, 48]               0
             GELU-20            [-1, 256, 2304]               0
          Dropout-21            [-1, 256, 2304]               0           
           Linear-10             [-1, 256, 576]         332,352

           Linear-22             [-1, 256, 576]       1,327,680        
        LayerNorm-11             [-1, 256, 576]           1,152

          Dropout-23             [-1, 256, 576]               0
           Linear-12            [-1, 256, 1728]         995,328
      FeedForward-24             [-1, 256, 576]               0
          Softmax-13          [-1, 8, 256, 256]               0
          PreNorm-25             [-1, 256, 576]               0
           Linear-14             [-1, 256, 576]         332,352
        LayerNorm-26            [-1, 225, 1152]           2,304
          Dropout-15             [-1, 256, 576]               0
           Linear-27             [-1, 225, 288]         332,064
    SelfAttention-16             [-1, 256, 576]               0
             GELU-28             [-1, 225, 288]               0
          PreNorm-17             [-1, 256, 576]               0
           Linear-29             [-1, 225, 288]          83,232
        LayerNorm-18             [-1, 256, 576]           1,152
        LayerNorm-30             [-1, 256, 288]             576
        LayerNorm-31             [-1, 225, 288]             576           
           Linear-19            [-1, 256, 2304]       1,329,408

           Linear-32             [-1, 256, 288]          82,944             
             GELU-20            [-1, 256, 2304]               0

           Linear-33             [-1, 225, 576]         165,888          
          Dropout-21            [-1, 256, 2304]               0

          Softmax-34          [-1, 4, 256, 225]               0
           Linear-22             [-1, 256, 576]       1,327,680
           Linear-35             [-1, 256, 288]          83,232          
          Dropout-23             [-1, 256, 576]               0

          Dropout-36             [-1, 256, 288]               0      
      FeedForward-24             [-1, 256, 576]               0

   CrossAttention-37             [-1, 256, 288]               0
          PreNorm-25             [-1, 256, 576]               0
         PreNorm2-38             [-1, 256, 288]               0
        LayerNorm-39             [-1, 225, 288]             576
        LayerNorm-26            [-1, 225, 1152]           2,304
        LayerNorm-40             [-1, 256, 288]             576
           Linear-27             [-1, 225, 288]         332,064
           Linear-41             [-1, 225, 288]          82,944
           Linear-42             [-1, 256, 576]         165,888            
             GELU-28             [-1, 225, 288]               0

          Softmax-43          [-1, 4, 225, 256]               0
           Linear-29             [-1, 225, 288]          83,232
           Linear-44             [-1, 225, 288]          83,232
        LayerNorm-30             [-1, 256, 288]             576          
          Dropout-45             [-1, 225, 288]               0

   CrossAttention-46             [-1, 225, 288]               0       
        LayerNorm-31             [-1, 225, 288]             576

         PreNorm2-47             [-1, 225, 288]               0
           Linear-32             [-1, 256, 288]          82,944
        LayerNorm-48             [-1, 225, 288]             576
           Linear-33             [-1, 225, 576]         165,888
           Linear-49             [-1, 225, 288]          83,232
          Softmax-34          [-1, 4, 256, 225]               0
             GELU-50             [-1, 225, 288]               0
           Linear-35             [-1, 256, 288]          83,232
           Linear-51            [-1, 225, 1152]         332,928
          Dropout-36             [-1, 256, 288]               0
        LayerNorm-52             [-1, 256, 576]           1,152
   CrossAttention-37             [-1, 256, 288]               0
           Linear-53            [-1, 256, 2304]       1,329,408
         PreNorm2-38             [-1, 256, 288]               0
             GELU-54            [-1, 256, 2304]               0
        LayerNorm-39             [-1, 225, 288]             576
          Dropout-55            [-1, 256, 2304]               0
        LayerNorm-40             [-1, 256, 288]             576
           Linear-56             [-1, 256, 576]       1,327,680
           Linear-41             [-1, 225, 288]          82,944
          Dropout-57             [-1, 256, 576]               0
           Linear-42             [-1, 256, 576]         165,888
      FeedForward-58             [-1, 256, 576]               0
          Softmax-43          [-1, 4, 225, 256]               0
          PreNorm-59             [-1, 256, 576]               0
           Linear-44             [-1, 225, 288]          83,232
           Conv2d-60           [-1, 64, 48, 48]          36,928
          Dropout-45             [-1, 225, 288]               0
             ReLU-61           [-1, 64, 48, 48]               0
   CrossAttention-46             [-1, 225, 288]               0
           Conv2d-62           [-1, 64, 48, 48]          36,928
         PreNorm2-47             [-1, 225, 288]               0
AdaptiveAvgPool2d-63             [-1, 64, 1, 1]               0
        LayerNorm-48             [-1, 225, 288]             576
           Conv2d-64              [-1, 4, 1, 1]             260
           Linear-49             [-1, 225, 288]          83,232
             ReLU-65              [-1, 4, 1, 1]               0
             GELU-50             [-1, 225, 288]               0
           Conv2d-66             [-1, 64, 1, 1]             320
           Linear-51            [-1, 225, 1152]         332,928
          Sigmoid-67             [-1, 64, 1, 1]               0
        LayerNorm-52             [-1, 256, 576]           1,152
          CALayer-68           [-1, 64, 48, 48]               0
           Linear-53            [-1, 256, 2304]       1,329,408
             RCAB-69           [-1, 64, 48, 48]               0
             GELU-54            [-1, 256, 2304]               0
           Conv2d-70           [-1, 64, 48, 48]          36,928
          Dropout-55            [-1, 256, 2304]               0
             ReLU-71           [-1, 64, 48, 48]               0
           Linear-56             [-1, 256, 576]       1,327,680
           Conv2d-72           [-1, 64, 48, 48]          36,928
          Dropout-57             [-1, 256, 576]               0
AdaptiveAvgPool2d-73             [-1, 64, 1, 1]               0
      FeedForward-58             [-1, 256, 576]               0
           Conv2d-74              [-1, 4, 1, 1]             260
          PreNorm-59             [-1, 256, 576]               0
             ReLU-75              [-1, 4, 1, 1]               0
           Conv2d-60           [-1, 64, 48, 48]          36,928
           Conv2d-76             [-1, 64, 1, 1]             320
             ReLU-61           [-1, 64, 48, 48]               0
          Sigmoid-77             [-1, 64, 1, 1]               0
           Conv2d-62           [-1, 64, 48, 48]          36,928
          CALayer-78           [-1, 64, 48, 48]               0
AdaptiveAvgPool2d-63             [-1, 64, 1, 1]               0
             RCAB-79           [-1, 64, 48, 48]               0
           Conv2d-64              [-1, 4, 1, 1]             260
           Conv2d-80           [-1, 64, 48, 48]          36,928
             ReLU-65              [-1, 4, 1, 1]               0
             ReLU-81           [-1, 64, 48, 48]               0
           Conv2d-82           [-1, 64, 48, 48]          36,928
           Conv2d-66             [-1, 64, 1, 1]             320
AdaptiveAvgPool2d-83             [-1, 64, 1, 1]               0
          Sigmoid-67             [-1, 64, 1, 1]               0
           Conv2d-84              [-1, 4, 1, 1]             260
             ReLU-85              [-1, 4, 1, 1]               0
          CALayer-68           [-1, 64, 48, 48]               0
           Conv2d-86             [-1, 64, 1, 1]             320
             RCAB-69           [-1, 64, 48, 48]               0
          Sigmoid-87             [-1, 64, 1, 1]               0
           Conv2d-70           [-1, 64, 48, 48]          36,928          
          CALayer-88           [-1, 64, 48, 48]               0

             RCAB-89           [-1, 64, 48, 48]               0
             ReLU-71           [-1, 64, 48, 48]               0
           Conv2d-90           [-1, 64, 48, 48]          36,928
           Conv2d-72           [-1, 64, 48, 48]          36,928
    ResidualGroup-91           [-1, 64, 48, 48]               0
AdaptiveAvgPool2d-73             [-1, 64, 1, 1]               0          
           Conv2d-92          [-1, 128, 48, 48]          16,384

             ReLU-93          [-1, 128, 48, 48]               0
           Conv2d-74              [-1, 4, 1, 1]             260
           Conv2d-94          [-1, 128, 48, 48]          16,384
             ReLU-75              [-1, 4, 1, 1]               0
               FB-95          [-1, 128, 48, 48]               0
           Conv2d-96          [-1, 128, 48, 48]          16,384
           Conv2d-76             [-1, 64, 1, 1]             320
             ReLU-97          [-1, 128, 48, 48]               0
          Sigmoid-77             [-1, 64, 1, 1]               0
           Conv2d-98          [-1, 128, 48, 48]          16,384
               FB-99          [-1, 128, 48, 48]               0          
          CALayer-78           [-1, 64, 48, 48]               0

          Conv2d-100          [-1, 128, 48, 48]          16,384
             RCAB-79           [-1, 64, 48, 48]               0
            ReLU-101          [-1, 128, 48, 48]               0
           Conv2d-80           [-1, 64, 48, 48]          36,928
          Conv2d-102          [-1, 128, 48, 48]          16,384
              FB-103          [-1, 128, 48, 48]               0
             ReLU-81           [-1, 64, 48, 48]               0
          Conv2d-104          [-1, 128, 48, 48]          16,384
            ReLU-105          [-1, 128, 48, 48]               0          
           Conv2d-82           [-1, 64, 48, 48]          36,928

          Conv2d-106          [-1, 128, 48, 48]          16,384
AdaptiveAvgPool2d-83             [-1, 64, 1, 1]               0
              FB-107          [-1, 128, 48, 48]               0
           Conv2d-84              [-1, 4, 1, 1]             260
          Conv2d-108           [-1, 64, 48, 48]          73,792
             ReLU-85              [-1, 4, 1, 1]               0
          Conv2d-109            [-1, 4, 48, 48]           2,308
           Conv2d-86             [-1, 64, 1, 1]             320
             ACT-110            [-1, 4, 48, 48]               0
          Sigmoid-87             [-1, 64, 1, 1]               0
    ACTLitModule-111            [-1, 4, 48, 48]               0
          CALayer-88           [-1, 64, 48, 48]               0
             RCAB-89           [-1, 64, 48, 48]               0
           Conv2d-90           [-1, 64, 48, 48]          36,928
    ResidualGroup-91           [-1, 64, 48, 48]               0
           Conv2d-92          [-1, 128, 48, 48]          16,384
             ReLU-93          [-1, 128, 48, 48]               0

           Conv2d-94          [-1, 128, 48, 48]          16,384
               FB-95          [-1, 128, 48, 48]               0
           Conv2d-96          [-1, 128, 48, 48]          16,384
             ReLU-97          [-1, 128, 48, 48]               0

           Conv2d-98          [-1, 128, 48, 48]          16,384
               FB-99          [-1, 128, 48, 48]               0
          Conv2d-100          [-1, 128, 48, 48]          16,384
            ReLU-101          [-1, 128, 48, 48]               0
          Conv2d-102          [-1, 128, 48, 48]          16,384
              FB-103          [-1, 128, 48, 48]               0
          Conv2d-104          [-1, 128, 48, 48]          16,384
            ReLU-105          [-1, 128, 48, 48]               0
          Conv2d-106          [-1, 128, 48, 48]          16,384
              FB-107          [-1, 128, 48, 48]               0
          Conv2d-108           [-1, 64, 48, 48]          73,792
          Conv2d-109            [-1, 4, 48, 48]           2,308
             ACT-110            [-1, 4, 48, 48]               0
    ACTLitModule-111            [-1, 4, 48, 48]               0
================================================================
Total params: 9,358,064
Trainable params: 9,358,064
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 81.00
Forward/backward pass size (MB): 138.91
Params size (MB): 35.70
Estimated Total Size (MB): 255.61
----------------------------------------------------------------
```

